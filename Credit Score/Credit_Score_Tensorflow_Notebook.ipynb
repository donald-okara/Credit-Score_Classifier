{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                            Credit Score Tensorflow Notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plotly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "df =  pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print(\"Data imported successfully....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column in Complaints_df\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# look at the # of missing points\n",
    "missing_values_count\n",
    "\n",
    "##Relevant Null vallues dropped in mining via query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing data handling plan\n",
    "1. Name: We wont be using this as a feature \n",
    "2. Monthly inhand salary,Number of delayed payment, Num credit inquiries, Amount invested monthly, Monthly balance : all set to zero.\n",
    "3. Type of Loan: will be split into one hot and null values will sort theselves out.\n",
    "4. Credit history age will be feature engineered into numerics and null will be set to zero\n",
    "\n",
    "There is data for eight months of the year for each client. Upon research, I found that credit score is not dependant on the previous month hence each entry in the dataset can and will be used independently. Trying a time series trick would be overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric values handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit history age to months\n",
    "def age_to_months(age_str):\n",
    "    if pd.isna(age_str) or age_str == '':\n",
    "        return float('nan')\n",
    "    age_list = age_str.split()\n",
    "    years = int(age_list[0])\n",
    "    months = int(age_list[3])\n",
    "    total_months = years * 12 + months\n",
    "    return total_months\n",
    "\n",
    "df['Credit_History_Age'] = df['Credit_History_Age'].apply(age_to_months)\n",
    "\n",
    "#Cleaning up underscored columns\n",
    "numeric_cols = ['Age','Annual_Income','Monthly_Inhand_Salary','Num_Bank_Accounts','Num_Credit_Card','Interest_Rate','Num_of_Loan','Delay_from_due_date','Num_of_Delayed_Payment','Changed_Credit_Limit','Num_Credit_Inquiries','Outstanding_Debt','Credit_Utilization_Ratio','Credit_History_Age','Total_EMI_per_month','Amount_invested_monthly','Monthly_Balance']\n",
    "categorical_cols = ['Occupation','Credit_Mix','Payment_of_Min_Amount','Payment_Behaviour', 'Credit_Score']\n",
    "\n",
    "# remove '_' from values in specified columns and convert to float\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_cols:\n",
    "    df[col] = df[col].replace('_', '')\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# print the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Categorical values handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Segregate into high paying, medium and low paying' and use ordinal encoding\n",
    "2. Month to be transformed into ordinal encoding\n",
    "3. Separate types of loans and Payment_Behaviour by commas and count number of loans\n",
    "4. Transform Credit Mix and Payment_of_Min_Amount with ordinal encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual salaries feature engineering\n",
    "\n",
    "# Define the income thresholds for each category\n",
    "income_bins = [0, 30000, 70000, float('inf')]\n",
    "income_labels = ['Low Income', 'Medium Income', 'High Income']\n",
    "\n",
    "# Segregate the annual income into categories using pandas cut() function\n",
    "df['Income_Category'] = pd.cut(df['Annual_Income'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Plot the frequency distribution of each income category using a bar plot\n",
    "df['Income_Category'].value_counts().plot(kind='bar', rot=0)\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Income_Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Annual Income by Category')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['Month','Occupation','Type_of_Loan','Credit_Mix','Payment_of_Min_Amount','Payment_Behaviour','Credit_Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['Payment_of_Min_Amount'].value_counts()\n",
    "print(unique_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot for each numeric column by \"Credit_Score\"\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    sns.boxplot(x='Credit_Score', y=col, data=df)\n",
    "    plt.title(col)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for each categorical column by \"Credit_Score\"\n",
    "# Iterate over each categorical column and create a bar plot\n",
    "for col in categorical_cols:\n",
    "    # Group the DataFrame by 'Credit_Score' and the current categorical column\n",
    "    grouped = df.groupby(['Credit_Score', col]).size().unstack()\n",
    "    \n",
    "    # Normalize the data to convert counts to proportions\n",
    "    grouped = grouped.div(grouped.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Create the bar plot\n",
    "    grouped.plot(kind='bar', stacked=True)\n",
    "    plt.title(col)\n",
    "    plt.xlabel('Credit_Score')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 70000, Test Set: 30000 \n",
      "\n",
      "X_train_Transformed /n [[-9.62948806e-01  1.83819055e-02 -4.77111390e-01 ...  2.00000000e+00\n",
      "   2.00000000e+00  4.00000000e+00]\n",
      " [-4.15967739e-02  0.00000000e+00  1.67810664e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  2.00000000e+00]\n",
      " [ 2.98216011e+00 -2.99535488e-02 -1.21796759e+00 ...  1.00000000e+00\n",
      "   1.00000000e+00  2.00000000e+00]\n",
      " ...\n",
      " [-3.83742770e-01 -1.38417307e-02 -1.42001928e+00 ...  1.00000000e+00\n",
      "   1.00000000e+00  4.00000000e+00]\n",
      " [-3.10528121e-16 -4.60653669e-02  2.63744806e-01 ...  1.00000000e+00\n",
      "   1.00000000e+00  4.00000000e+00]\n",
      " [-1.14982457e+00  8.28291779e-02 -4.09760827e-01 ...  0.00000000e+00\n",
      "   2.00000000e+00  6.00000000e+00]]\n",
      "X_test_Transformed /n [[-3.10528121e-16  6.67173598e-02  1.27400326e+00 ...  0.00000000e+00\n",
      "   2.00000000e+00  2.00000000e+00]\n",
      " [-2.60029570e-01  3.44937236e-02 -4.09760827e-01 ...  0.00000000e+00\n",
      "   2.00000000e+00  6.00000000e+00]\n",
      " [-3.10528121e-16 -2.99535488e-02  2.63744806e-01 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " ...\n",
      " [-4.56865826e-01 -1.38417307e-02 -7.46513644e-01 ...  3.00000000e+00\n",
      "   1.00000000e+00  6.00000000e+00]\n",
      " [-4.01074005e-01 -1.38417307e-02  1.67810664e+00 ...  2.00000000e+00\n",
      "   2.00000000e+00  6.00000000e+00]\n",
      " [ 3.12121664e-02  8.28291779e-02  9.37250439e-01 ...  0.00000000e+00\n",
      "   2.00000000e+00  2.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and label\n",
    "features = ['Monthly_Inhand_Salary', 'Num_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit',  'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Credit_History_Age', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Monthly_Balance', 'Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour']\n",
    "label = 'Credit_Score'\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[label], test_size=0.3, random_state=0)\n",
    "\n",
    "# Define a pipeline for numeric columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define a pipeline for categorical columns\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# Create a ColumnTransformer to apply the pipeline to the numeric and categorical columns\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, X_train.select_dtypes(include=['int64', 'float64']).columns),\n",
    "    ('cat', cat_transformer, X_train.select_dtypes(include=['object']).columns)\n",
    "])\n",
    "\n",
    "# Fit the preprocessor to the training data and transform both the training and test data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "print('Training Set: %d, Test Set: %d \\n' % (len(X_train), len(X_test)))\n",
    "\n",
    "# Print the transformed DataFrame\n",
    "print(\"X_train_Transformed /n\",X_train_transformed)\n",
    "print(\"X_test_Transformed /n\",X_test_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\DATASETS & PROJECTS\\Credit Score\\Credit_Score_Tensorflow_Notebook.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DATASETS%20%26%20PROJECTS/Credit%20Score/Credit_Score_Tensorflow_Notebook.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DATASETS%20%26%20PROJECTS/Credit%20Score/Credit_Score_Tensorflow_Notebook.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Assuming y_test is a pandas Series object\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DATASETS%20%26%20PROJECTS/Credit%20Score/Credit_Score_Tensorflow_Notebook.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# First, convert it to a NumPy array\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DATASETS%20%26%20PROJECTS/Credit%20Score/Credit_Score_Tensorflow_Notebook.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_test_np \u001b[39m=\u001b[39m y_test\u001b[39m.\u001b[39mto_numpy()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming y_test is a pandas Series object\n",
    "# First, convert it to a NumPy array\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "# Calculate the number of unique classes in y_test\n",
    "num_classes = len(set(y_test_np))\n",
    "\n",
    "# Transform y_test to ordinal labels\n",
    "y_test_transformed = tf.keras.utils.to_categorical(y_test_np, num_classes)\n",
    "\n",
    "print(\"y_test transformed to ordinal labels:\")\n",
    "print(y_test_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
